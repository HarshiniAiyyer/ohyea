# -*- coding: utf-8 -*-
"""fin2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/119Fv-NpkdGlqui3XpS_2amZM4uqA4Lua
"""

#pip install -q opendatasets benfordslaw bokeh

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import opendatasets as od

url = "https://raw.githubusercontent.com/HarshiniAiyyer/Financial-Forensics/refs/heads/main/states.csv"
od.download(url)

#username - harshiniaiyyer
#password - db9694435b6d0b386a9c18289bb0f443

"""### Reading file"""

df = pd.read_csv('/content/health-insurance/states.csv')
df.head()

df.isnull().sum()

df = df.dropna()
df = df.drop(columns = ['Uninsured Rate Change (2010-2015)'])

# Remove the percentages and dollar signs
def clean_percentage(value):
    if isinstance(value, str):
        if "%" in value:
            return value.replace('%', '')
        elif "$" in value:
            return value.replace('$', '').replace(',', '')
    return value

# Apply the cleaning function to all columns
df = df.map(clean_percentage)
df.head()

df.info()

# Loop through columns (excluding the first column) and convert 'object' columns to float
for col in df.columns[1:]:  # Exclude the first column by starting from index 1
    if df[col].dtype == 'object':  # Check if the column has 'object' type
        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric (float), set errors to NaN if conversion fails

# Print the updated DataFrame
df.head()

df.info()

"""### Benfords Law

#### First digit test
"""

from benfordslaw import benfordslaw

# Initialize
bl1 = benfordslaw(alpha=0.05)

x = np.array(df.iloc[:,12].values)

# Make fit
first_digit = bl1.fit(x)

# Plot
bl1.plot(title='Medicare Enrollment (2016)')

"""### Second digit test"""

# Initialize
bl2 = benfordslaw(pos=2)

# Make fit
sec_digit = bl2.fit(x)

# Plot
bl2.plot(title='Medicare Enrollment (2016) - Benfords Law Second Digit Test',
         barcolor=[0.5, 0.5, 0.5], fontsize=12, barwidth=0.4)

"""#### Last digit Benford's Test"""

# Initialize
bllast = benfordslaw(pos=-1)

# Make fit
last_digit = bllast.fit(x)

# Plot
bllast.plot(title='Medicare Enrollment (2016) - Benfords Law Last Digit Test')

"""### Benford Law for Second Last Digit"""

# Initialize
blseclast = benfordslaw(pos=-2)

# Make fit
second_last_digit = blseclast.fit(x)

# Plot
blseclast.plot(title='Medicare Enrollment (2016) - Benfords Law Second Last Digit Test')

df.info()


"""### ML algorithms Pipeline

#### Data Setup
"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()


x = df.iloc[:,[3,4,5,6,7,9,10,11,12]].values

y = le.fit_transform(df.iloc[:,8])



from sklearn.preprocessing import StandardScaler

# the scaler object (model)
scaler = StandardScaler()
# fit and transform the data
x = scaler.fit_transform(x)

"""#### Train and Test Split"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

"""#### Model Pipeline"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier


pipe = []
pipe.append(LogisticRegression(solver = 'liblinear'))
pipe.append(SVC(kernel = 'linear',gamma = 'scale', shrinking = False))
pipe.append(KNeighborsClassifier())
pipe.append(DecisionTreeClassifier())
pipe.append(RandomForestClassifier())
pipe.append(GaussianNB())
pipe.append(MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1))
pipe.append(XGBClassifier(max_depth=5, learning_rate=0.01, n_estimators=100, gamma=0,
                        min_child_weight=1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005))
pipe.append(AdaBoostClassifier(random_state = 96))

"""#### Confusion Matrices"""

from sklearn import metrics

from sklearn.metrics import classification_report,confusion_matrix

mod = ['Logistic Regression','SVM','KNN','Decision Tree','Random Forest','Naive Bayes','MLP Classifier','XGBoost','AdaBoost']
acclist = []
auclist = []

cmlist = []

for i in pipe:
  i.fit(x_train,y_train)
  y_pred = i.predict(x_test)
  acclist.append(metrics.accuracy_score(y_test,y_pred))
  fpr,tpr, thresh = metrics.roc_curve(y_test, y_pred)
  auclist.append(round(metrics.auc(fpr,tpr),2))
  cmlist.append(confusion_matrix(y_test,y_pred))

import seaborn as sns
#plotting confusion matrix


fig = plt.figure(figsize = (15,15))

for i in range(len(cmlist)):
  cm = cmlist[i]
  model = mod[i]
  sub = fig.add_subplot(3,3,i+1).set_title(model)

  cmplot = sns.heatmap(cm,annot = True, cmap = 'Blues_r')
  cmplot.set_xlabel('Predicted Values')
  cmplot.set_ylabel('Actual Values')

"""#### Tabulating accuracies"""

res = pd.DataFrame({'Model':mod, 'Accuracy':acclist,'AUC':auclist})
res

"""#### Barplot for accuracies"""

dark = sns.dark_palette("seagreen", reverse=True, n_colors=len(res))

# Create the barplot
sns.barplot(x="Model", y="Accuracy", data=res, palette=dark, hue = None)

plt.xticks(rotation=90)
plt.title('Accuracy Comparison')
plt.show()

